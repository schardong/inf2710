"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"2MN4UWVG","journalArticle","2011","Oh, Sangyoon; Lee, Min Su; Zhang, Byoung-Tak","Ensemble Learning with Active Example Selection for Imbalanced Biomedical Data Classification","IEEE/ACM Trans. Comput. Biol. Bioinformatics","","1545-5963","10.1109/TCBB.2010.96","http://dx.doi.org/10.1109/TCBB.2010.96","In biomedical data, the imbalanced data problem occurs frequently and causes poor prediction performance for minority classes. It is because the trained classifiers are mostly derived from the majority class. In this paper, we describe an ensemble learning method combined with active example selection to resolve the imbalanced data problem. Our method consists of three key components: 1) an active example selection algorithm to choose informative examples for training the classifier, 2) an ensemble learning method to combine variations of classifiers derived by active example selection, and 3) an incremental learning scheme to speed up the iterative training procedure for active example selection. We evaluate the method on six real-world imbalanced data sets in biomedical domains, showing that the proposed method outperforms both the random under sampling and the ensemble with under sampling methods. Compared to other approaches to solving the imbalanced data problem, our method excels by 0.03-0.15 points in AUC measure.","2011-03","2016-04-05 13:28:15","2016-04-05 13:28:15","2016-04-05 13:28:15","316–325","","2","8","","","","","","","","","","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/28HGRWD2/Oh et al. - 2011 - Ensemble Learning with Active Example Selection fo.pdf","","","Bioinformatics; Classification; interactive data exploration and discovery; mining methods and algorithms.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4952CIGK","conferencePaper","2006","Card, Stuart W.; Mohan, Chilukuri K.","Ensemble Selection for Evolutionary Learning Using Information Theory and Price's Theorem","Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation","978-1-59593-186-3","","10.1145/1143997.1144254","http://doi.acm.org/10.1145/1143997.1144254","This paper presents an information theoretic perspective on design and analysis of evolutionary algorithms. Indicators of solution quality are developed and applied not only to individuals but also to ensembles, thereby ensuring information diversity. Price's Theorem is extended to show how joint indicators can drive reproductive sampling rate of potential parental pairings. Heritability of mutual information is identified as a key issue.","2006","2016-04-05 13:08:26","2016-04-05 13:08:26","2016-04-05 13:08:26","1587–1588","","","","","","","GECCO '06","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/C58KRP4P/Card e Mohan - 2006 - Ensemble Selection for Evolutionary Learning Using.pdf","","","ensemble models; evolutionary computation; group selection; Machine learning; mate selection; Price's Equation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4THKQ3IW","conferencePaper","2005","Buchholz, Peter; Thümmler, Axel","Enhancing Evolutionary Algorithms with Statistical Selection Procedures for Simulation Optimization","Proceedings of the 37th Conference on Winter Simulation","978-0-7803-9519-0","","","http://dl.acm.org/citation.cfm?id=1162708.1162855","In this paper, we present an evolution strategy for the optimization of simulation models. Our approach incorporates statistical selection procedures that efficiently select the best individual, where best is defined by the maximum or minimum expected simulation response. We use statistical procedures for the survivor selection during the evolutionary process and for selecting the best individual from a set of candidate best individuals, a so-called elite population, at the end of the evolutionary process. Furthermore, we propose a heuristic selection procedure that reduces a random-size subset, containing the best individual, to at most a predefined size. By means of a stochastic sphere function and a simulation model of a production line, we show that this procedure performs better in terms of number of model evaluations and solution quality than other state-of-the-art statistical selection procedures.","2005","2016-04-05 13:35:55","2016-04-05 15:07:56","2016-04-05 13:35:55","842–852","","","","","","","WSC '05","","","","Winter Simulation Conference","Orlando, Florida","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/RFBD5Q3B/Buchholz e Thümmler - 2005 - Enhancing Evolutionary Algorithms with Statistical.pdf; /home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/RGWRCUV2/display.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7D3EPAK7","journalArticle","2014","Zheng, Li; Li, Tao; Ding, Chris","A Framework for Hierarchical Ensemble Clustering","ACM Trans. Knowl. Discov. Data","","1556-4681","10.1145/2611380","http://doi.acm.org/10.1145/2611380","Ensemble clustering, as an important extension of the clustering problem, refers to the problem of combining different (input) clusterings of a given dataset to generate a final (consensus) clustering that is a better fit in some sense than existing clusterings. Over the past few years, many ensemble clustering approaches have been developed. However, most of them are designed for partitional clustering methods, and few research efforts have been reported for ensemble hierarchical clustering methods. In this article, a hierarchical ensemble clustering framework that can naturally combine both partitional clustering and hierarchical clustering results is proposed. In addition, a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation is developed based on the ultra-metric distance for hierarchical clustering. We study three important problems: dendrogram description, dendrogram combination, and dendrogram selection. We develop two approaches for dendrogram selection based on tree distances, and we investigate various dendrogram distances for representing dendrograms. We provide a systematic empirical study of the ensemble hierarchical clustering problem. Experimental results demonstrate the effectiveness of our proposed approaches.","2014-09","2016-04-05 13:08:26","2016-04-05 13:33:08","2016-04-05 13:08:26","9:1–9:23","","2","9","","","","","","","","","","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/VR99PC44/Zheng et al. - 2014 - A Framework for Hierarchical Ensemble Clustering.pdf","","ensemble selection; Hierarchical ensemble clustering; ultra-metric","ensemble selection; Hierarchical ensemble clustering; ultra-metric","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ERM2TGX","conferencePaper","2004","Caruana, Rich; Niculescu-Mizil, Alexandru; Crew, Geoff; Ksikes, Alex","Ensemble Selection from Libraries of Models","Proceedings of the Twenty-first International Conference on Machine Learning","978-1-58113-838-2","","10.1145/1015330.1015432","http://doi.acm.org/10.1145/1015330.1015432","We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.","2004","2016-04-05 13:28:15","2016-04-05 13:28:15","2016-04-05 13:28:15","18–","","","","","","","ICML '04","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/RZW8IEMW/Caruana et al. - 2004 - Ensemble Selection from Libraries of Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7KISGC9F","conferencePaper","2006","Lee, Loo Hay; Chew, Ek Peng; Teng, Suyan","Integration of Statistical Selection with Search Mechanism for Solving Multi-objective Simulation-optimization Problems","Proceedings of the 38th Conference on Winter Simulation","978-1-4244-0501-5","","","http://dl.acm.org/citation.cfm?id=1218112.1218170","In this paper, we consider a multi-objective simulation optimization problem with three features: huge solution space, high uncertainty in performance measures, and multi-objective problem which requires a set of nondominated solutions. Our main purpose is to study how to integrate statistical selection with search mechanism to address the above difficulties, and to present a general solution framework for solving such problems. Here due to the multi-objective nature, statistical selection is done by the multi-objective computing budget allocation (MOCBA) procedure. For illustration, MOCBA is integrated with two meta-heuristics: multi-objective evolutionary algorithm (MOEA) and nested partitions (NP) to identify the nondominated solutions for two inventory management case study problems. Results show that, the integrated solution framework has improved both search efficiency and simulation efficiency. Moreover, it is capable of identifying a set of non-dominated solutions with high confidence.","2006","2016-04-05 13:35:55","2016-04-05 15:08:04","2016-04-05 13:35:55","294–303","","","","","","","WSC '06","","","","Winter Simulation Conference","Monterey, California","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/TDD679VT/Lee et al. - 2006 - Integration of Statistical Selection with Search M.pdf; /home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/MRUCUVM9/display.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFINXEFU","conferencePaper","2012","Gullo, Francesco; Talukder, AKM Khaled; Luke, Sean; Domeniconi, Carlotta; Tagarelli, Andrea","Multiobjective Optimization of Co-clustering Ensembles","Proceedings of the 14th Annual Conference Companion on Genetic and Evolutionary Computation","978-1-4503-1178-6","","10.1145/2330784.2331010","http://doi.acm.org/10.1145/2330784.2331010","Co-clustering is a machine learning task where the goal is to simultaneously develop clusters of the data and of their respective features. We address the use of co-clustering ensembles to establish a consensus co-clustering over the data. In this paper we develop a new preference-based multiobjective optimization algorithm to compete with a previous gradient ascent approach in finding optimal co-clustering ensembles. Unlike the gradient ascent algorithm, our approach once tackles the co-clustering problem with multiple heuristics, then applies the gradient ascent algorithm's joint heuristic as a preference selection procedure. We are able to significantly outperform the gradient ascent algorithm on feature clustering and on problems with smaller datasets.","2012","2016-04-05 13:28:15","2016-04-05 13:33:52","2016-04-05 13:28:15","1495–1496","","","","","","","GECCO '12","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/XPCCIUZE/Gullo et al. - 2012 - Multiobjective Optimization of Co-clustering Ensem.pdf","","co-clustering; ensembles; multiobjective optimization","co-clustering; ensembles; multiobjective optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TPMAWKRX","conferencePaper","2010","Lu, Zhenyu; Wu, Xindong; Zhu, Xingquan; Bongard, Josh","Ensemble Pruning via Individual Contribution Ordering","Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","978-1-4503-0055-1","","10.1145/1835804.1835914","http://doi.acm.org/10.1145/1835804.1835914","An ensemble is a set of learned models that make decisions collectively. Although an ensemble is usually more accurate than a single learner, existing ensemble methods often tend to construct unnecessarily large ensembles, which increases the memory consumption and computational cost. Ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble. In this paper, we analyze the accuracy/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction. Based on the gained insights, a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier's contribution to the whole ensemble. By incorporating ensemble members in decreasing order of their contributions, subensembles are formed such that users can select the top $p$ percent of ensemble members, depending on their resource availability and tolerable waiting time, for predictions. Experimental results on 26 UCI data sets show that subensembles formed by the proposed EPIC (Ensemble Pruning via Individual Contribution ordering) algorithm outperform the original ensemble and a state-of-the-art ensemble pruning method, Orientation Ordering (OO).","2010","2016-04-04 11:37:20","2016-04-04 11:37:20","2016-04-04 11:37:20","871–880","","","","","","","KDD '10","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/V26JE7AT/Lu et al. - 2010 - Ensemble Pruning via Individual Contribution Order.pdf","","","ensemble learning; ensemble pruning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U4NECHEG","conferencePaper","2013","Elhadi, Haithum; Agam, Gady","Structure and Attributes Community Detection: Comparative Analysis of Composite, Ensemble and Selection Methods","Proceedings of the 7th Workshop on Social Network Mining and Analysis","978-1-4503-2330-7","","10.1145/2501025.2501034","http://doi.acm.org/10.1145/2501025.2501034","In recent years due to the rise of social, biological, and other rich content graphs, several new graph clustering methods using structure and node's attributes have been introduced. In this paper, we compare our novel clustering method, termed Selection method, against seven clustering methods: three structure and attribute methods, one structure only method, one attribute only method, and two ensemble methods. The Selection method uses the graph structure ambiguity to switch between structure and attribute clustering methods. We shows that the Selection method out performed the state-of-art structure and attribute methods.","2013","2016-04-05 13:28:15","2016-04-05 13:34:06","2016-04-05 13:28:15","10:1–10:7","","","","","","Structure and Attributes Community Detection","SNAKDD '13","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/7WCC98CB/Elhadi e Agam - 2013 - Structure and Attributes Community Detection Comp.pdf","","Community detection; social network analysis; structure and attributes clustering","Community detection; social network analysis; structure and attributes clustering","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJ6I88SN","conferencePaper","2015","Rastin, Parisa; Kanawati, Rushed","A Multiplex-network Based Approach for Clustering Ensemble Selection","Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015","978-1-4503-3854-7","","10.1145/2808797.2808825","http://doi.acm.org/10.1145/2808797.2808825","Performance of cluster ensemble approaches is now known to be tightly related to both quality and diversity of input base clusterings. Cluster ensemble selection (CES) refers to the process of filtering the raw set of base clusterings in order to select a subset of high quality and diverse clusterings. Most of existing CES approaches apply one index for measuring the quality and another for evaluating the diversity of clusterings. Moreover the number of clusterings to select is usually given as an input to the CES function. In this work we propose a new CES approach that allow taking into account an ensemble of quality and diversity indexes. In addition, the proposed approach computes automatically the number of clusterings to return. The basic idea is to define a multiplex network over the provided set of base clusterings. Each slice in the multiplex network is obtained by defining a proximity-graph over the set of base clusterings using a given clustering dissimilarity index. A community detection algorithm is applied to the obtained multiplex network. We then rank clusterings in each community applying an ensemble-ranking approach using different (internal) clustering quality indexes. From each community we select the base clustering ranked at the top. First experiments on benchmark datasets shows the effectiveness of the proposed CES approach.","2015","2016-04-05 13:08:26","2016-04-05 13:33:33","2016-04-05 13:08:26","1332–1339","","","","","","","ASONAM '15","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","/home/gschardong/.zotero/zotero/nah1hk19.default/zotero/storage/I78FCKFJ/Rastin e Kanawati - 2015 - A Multiplex-network Based Approach for Clustering .pdf","","Clustering ensemble selection; Community detection; ensemble clustering; Multiplex network","Clustering ensemble selection; Community detection; Ensemble clustering; Multiplex network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUDPZ2XM","conferencePaper","2013","Brochero, Darwin; Gagné, Christian; Anctil, François","Evolutionary Multiobjective Optimization for Selecting Members of an Ensemble Streamflow Forecasting Model","Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation","978-1-4503-1963-8","","10.1145/2463372.2463538","http://doi.acm.org/10.1145/2463372.2463538","We are proposing to use the Nondominated Sorting Genetic Algorithm II (NSGA-II) for optimizing a hydrological forecasting model of 800 simultaneous streamflow predictors. The optimization is based on the selection of the best 48 predictors from the 800 that jointly define the ""best"" ensemble in terms of two probabilistic criteria. Results showed that the difficulties in simplifying the ensembles mainly originate from the preservation of the system reliability. We conclude that Pareto fronts generated with NSGA-II allow the development of a decision process based explicitly on the trade-off between different probabilistic properties. In other words, evolutionary multiobjective optimization offers more flexibility to the operational hydrologists than a priori methods that produce only one selection.","2013","2016-04-04 11:37:20","2016-04-04 11:37:20","2016-04-04 11:37:20","1221–1228","","","","","","","GECCO '13","","","","ACM","New York, NY, USA","","","","","","ACM Digital Library","","","","","http://dl.acm.org/ft_gateway.cfm?id=2463538&ftid=1381425&dwn=1&CFID=767509089&CFTOKEN=24979330","","evolutionary multiobjective optimization; hydrological ensemble prediction system; probabilistic forecasting; streamflow forecasting; uncertainty cascade model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""